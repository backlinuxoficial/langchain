from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate  # ‚úÖ CORRIGIDO
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
import os
import time

# Carrega o Livro
loader = PyPDFLoader("A-ARTE-DA-GUERRA.pdf")
documents = loader.load()
print(f"Total de p√°ginas: {len(documents)}")

# Divide em chunks
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\n\n", "\n", " ", ""]
)
chunks = splitter.split_documents(documents)
print(f"Total de chunks: {len(chunks)}")

# Embeddings locais (gratuito)
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

# Teste de dimens√µes
v = embeddings.embed_query("machine learning")
print(f"Dimens√µes: {len(v)}")

# Cria/carrega o banco vetorial
vectorstore = Chroma(
    collection_name="arte_guerra",
    embedding_function=embeddings,
    persist_directory="./chroma.db"
)

n = vectorstore._collection.count()
print(f"Vetores j√° indexados: {n}")

# ‚úÖ L√ìGICA CORRIGIDA: s√≥ popula se estiver vazio
if n == 0:  # ‚úÖ SE for zero, ent√£o precisa popular
    print("Populando banco vetorial...")
    batch_size = 20
    for i in range(0, len(chunks), batch_size):
        batch = chunks[i:i+batch_size]
        try:
            vectorstore.add_documents(batch)
            print(f"Lote {i//batch_size+1} indexado com {len(batch)} documentos")
            time.sleep(1)  # pausa preventiva
        except Exception as e:
            print(f"Erro: {e}")
            time.sleep(45)
    print("Popula√ß√£o conclu√≠da!")
else:
    print("Banco j√° populado. Pulando indexa√ß√£o.")

# Pergunta do usu√°rio
query = "Qual a estrat√©gia definida no livro que pode ser aplicada no cotidiano?"

# Retriever
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 4}
)

# Recupera os 4 chunks mais relevantes
docs = retriever.invoke(query)
print(f"\nüîç {len(docs)} chunks recuperados:\n")
for i, doc in enumerate(docs):
    pag = doc.metadata.get('page', '?')
    print(f"Chunk {i+1} - p√°g. {pag}:")
    print(doc.page_content[:150])
    print()

# ‚úÖ PROMPT CORRIGIDO
prompt_template = ChatPromptTemplate.from_messages([
    ("system", """Voc√™ √© um assistente especialista.
Use APENAS o contexto abaixo para responder.
Se n√£o estiver no contexto, diga isso claramente.

Contexto: {context}"""),  # ‚úÖ placeholder "context" (n√£o "contexto")
    ("human", "{question}")
])

# Gemini Pro
os.environ["GOOGLE_API_KEY"] = "Sua API"
llm = ChatGoogleGenerativeAI(
    model="gemini-3-flash-preview",
    temperature=0.3
)

# Chain LCEL
rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}  # ‚úÖ "context" consistente
    | prompt_template
    | llm
    | StrOutputParser()
)

# Invoca a chain
print("\nü§ñ Gerando resposta...\n")
print("="*60)
for chunk in rag_chain.stream(query):
    print(chunk, end="", flush=True)
print("\n" + "="*60)